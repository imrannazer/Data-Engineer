C:\Users\pc>pyspark
Python 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018, 14:57:15) [MSC v.1915 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
23/10/22 19:18:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\pyspark\context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
  warnings.warn("Python 3.7 support is deprecated in Spark 3.4.", FutureWarning)
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.4.1
      /_/

Using Python version 3.7.1 (v3.7.1:260ec2c36a, Oct 20 2018 14:57:15)
Spark context Web UI available at http://LAPTOP-2LC0K32A:4040
Spark context available as 'sc' (master = local[*], app id = local-1697982526272).
SparkSession available as 'spark'.
>>>
>>> numrdd = sc.parallelize([5,6,9,8,5,45,6,2,5,6,3,2,5])
>>> strrdd = sc.parallelize(['kafka hadoop','hadoop kafka','hive sqoop','pysql sqoop','hive kafka','hadoop'])
>>> numrdd.collect()
23/10/22 19:22:54 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes
[5, 6, 9, 8, 5, 45, 6, 2, 5, 6, 3, 2, 5]
>>> add10 = numrdd.map(lambda x: x+10)
>>> add10.collect()
[15, 16, 19, 18, 15, 55, 16, 12, 15, 16, 13, 12, 15]
>>> type(sc)
<class 'pyspark.context.SparkContext'>
>>> add10 = numrdd.map(lambda x: x*x)
>>> add10.collect()
[25, 36, 81, 64, 25, 2025, 36, 4, 25, 36, 9, 4, 25]
>>> datardd = strrdd.map(lambda x: x.upper())
>>> datardd.collect()
['KAFKA HADOOP', 'HADOOP KAFKA', 'HIVE SQOOP', 'PYSQL SQOOP', 'HIVE KAFKA', 'HADOOP']
>>> lowerrdd = datardd.map(lambda x:x.lower())
>>> lowerrdd.collect()
['kafka hadoop', 'hadoop kafka', 'hive sqoop', 'pysql sqoop', 'hive kafka', 'hadoop']
>>> def uppercase(x);
  File "<stdin>", line 1
    def uppercase(x);
                    ^
SyntaxError: invalid syntax
>>> def uppercase(x):
...  return x.upper()
...
>>> arrrdd
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'arrrdd' is not defined
>>> strrdd
ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:287
>>> upperfuncion = strrdd.map(lambda x : uppercase(x))
>>> upperfunction.collect()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'upperfunction' is not defined
>>> upperfuncion.collect()
['KAFKA HADOOP', 'HADOOP KAFKA', 'HIVE SQOOP', 'PYSQL SQOOP', 'HIVE KAFKA', 'HADOOP']
>>> def lowercase(x):
...  return x.lower()
...
>>> lowerfunction = strrdd.map(lambda x: lowercase(x))
>>> lowerfunction.collect()
['kafka hadoop', 'hadoop kafka', 'hive sqoop', 'pysql sqoop', 'hive kafka', 'hadoop']
>>> mrdd = strrdd.map(lambda x: x.split(" "))
>>> frdd = strrdd.flatMap(lambda x: x.split(" "))
>>> mrdd.collect()
[['kafka', 'hadoop'], ['hadoop', 'kafka'], ['hive', 'sqoop'], ['pysql', 'sqoop'], ['hive', 'kafka'], ['hadoop']]
>>> frdd.collect()
['kafka', 'hadoop', 'hadoop', 'kafka', 'hive', 'sqoop', 'pysql', 'sqoop', 'hive', 'kafka', 'hadoop']
>>> length(mrdd)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'length' is not defined
>>> mrdd.length()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'PipelinedRDD' object has no attribute 'length'
>>> len(mrdd)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: object of type 'PipelinedRDD' has no len()
>>> len(list(mrdd))
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'PipelinedRDD' object is not iterable
>>> mrdd.collect()
[['kafka', 'hadoop'], ['hadoop', 'kafka'], ['hive', 'sqoop'], ['pysql', 'sqoop'], ['hive', 'kafka'], ['hadoop']]
>>> frdd.collect()
['kafka', 'hadoop', 'hadoop', 'kafka', 'hive', 'sqoop', 'pysql', 'sqoop', 'hive', 'kafka', 'hadoop']
>>> numrdd.collect()
[5, 6, 9, 8, 5, 45, 6, 2, 5, 6, 3, 2, 5]
>>> evennum = numrdd.map(lambda x: x%2 == 0)
>>> evennum.collect()
[False, True, False, True, False, False, True, True, False, True, False, True, False]
>>> evenfnum = numrdd.flatMap(lambda x: x%2 == 0)
>>> evenfnum.collect()
23/10/22 19:52:55 ERROR Executor: Exception in task 6.0 in stage 13.0 (TID 110)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 830, in main
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 822, in process
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\serializers.py", line 274, in dump_stream
    vs = list(itertools.islice(iterator, batch))
TypeError: 'bool' object is not iterable

        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
        at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
        at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
        at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
        at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)
        at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
23/10/22 19:52:55 WARN TaskSetManager: Lost task 6.0 in stage 13.0 (TID 110) (LAPTOP-2LC0K32A executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 830, in main
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 822, in process
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\serializers.py", line 274, in dump_stream
    vs = list(itertools.islice(iterator, batch))
TypeError: 'bool' object is not iterable

        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
        at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
        at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
        at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
        at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)
        at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

23/10/22 19:52:55 ERROR TaskSetManager: Task 6 in stage 13.0 failed 1 times; aborting job
Traceback (most recent call last):                                  (0 + 7) / 8]
  File "<stdin>", line 1, in <module>
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\pyspark\rdd.py", line 1814, in collect
    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\py4j-0.10.9.7-src.zip\py4j\java_gateway.py", line 1323, in __call__
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\pyspark\errors\exceptions\captured.py", line 169, in deco    return f(*a, **kw)
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\py4j-0.10.9.7-src.zip\py4j\protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 13.0 failed 1 times, most recent failure: Lost task 6.0 in stage 13.0 (TID 110) (LAPTOP-2LC0K32A executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 830, in main
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 822, in process
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\serializers.py", line 274, in dump_stream
    vs = list(itertools.islice(iterator, batch))
TypeError: 'bool' object is not iterable

        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
        at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
        at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
        at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
        at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)
        at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)
        at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:405)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:1018)
        at org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)
        at org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 830, in main
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\worker.py", line 822, in process
  File "D:\Big-Data-2023\parmanent_location\spark-3.4.1\python\lib\pyspark.zip\pyspark\serializers.py", line 274, in dump_stream
    vs = list(itertools.islice(iterator, batch))
TypeError: 'bool' object is not iterable

        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
        at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
        at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
        at scala.collection.TraversableOnce.to(TraversableOnce.scala:366)
        at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)
        at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)
        at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)
        at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
        at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)
        at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)
        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)
        at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
        at org.apache.spark.scheduler.Task.run(Task.scala:139)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ... 1 more

>>> 23/10/22 19:52:56 WARN TaskSetManager: Lost task 1.0 in stage 13.0 (TID 105) (LAPTOP-2LC0K32A executor driver): TaskKilled (Stage cancelled)
23/10/22 19:52:56 WARN TaskSetManager: Lost task 2.0 in stage 13.0 (TID 106) (LAPTOP-2LC0K32A executor driver): TaskKilled (Stage cancelled)
23/10/22 19:52:57 WARN TaskSetManager: Lost task 5.0 in stage 13.0 (TID 109) (LAPTOP-2LC0K32A executor driver): TaskKilled (Stage cancelled)
23/10/22 19:52:57 WARN TaskSetManager: Lost task 3.0 in stage 13.0 (TID 107) (LAPTOP-2LC0K32A executor driver): TaskKilled (Stage cancelled)
23/10/22 19:52:58 WARN TaskSetManager: Lost task 7.0 in stage 13.0 (TID 111) (LAPTOP-2LC0K32A executor driver): TaskKilled (Stage cancelled)
23/10/22 19:52:58 WARN TaskSetManager: Lost task 4.0 in stage 13.0 (TID 108) (LAPTOP-2LC0K32A executor driver): TaskKilled (Stage cancelled)
23/10/22 19:52:59 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 104) (LAPTOP-2LC0K32A executor driver): TaskKilled (Stage cancelled)

>>> evenmap.collect()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'evenmap' is not defined
>>> evennum.collect()
[False, True, False, True, False, False, True, True, False, True, False, True, False]
>>> evennum = numrdd.filter(lambda x : x%2==0)
>>> evennum.collect()
[6, 8, 6, 2, 6, 2]
>>> oddrdd = numrdd.filter(lambda x: x%2==0)
>>> oddrdd.collect()
[6, 8, 6, 2, 6, 2]
>>> oddrdd = numrdd.filter(lambda x: x%2!=0)
>>> oddrdd.collect()
[5, 9, 5, 45, 5, 3, 5]
>>> data = sc.textFile("file:///D:\\Big-Data-2023\\Spark\\data.txt")
>>> data.collect()
['apache kafka flume karka hive ']
>>> data.collect()
['apache kafka flume karka hive ']
>>> data = sc.textFile("file:///D:\\Big-Data-2023\\Spark\\data.txt")
>>> oddrdd.collect()
[5, 9, 5, 45, 5, 3, 5]
>>> data.collect()
['apache kafka flume karka hive ', 'kafka spark sqoop hive hadoop sqoop kafka sqoop hive spark sqoop kafka spark sqoop hadoop kafka spark sqoop hadoop hive sqoop hadoop spark kafka sqoop hadoop kafka spark sqoop hadoop kafka spark hive sqoop ', 'spark hadoop kafka sqoop hive kafka spark sqoop kafka spark sqoop hadoop kafka sqoop hadoop spark sqoop hadoop kafka sqoop spark kafka sqoop hive kafka spark ', 'sqoop kafka spark sqoop hadoop kafka sqoop hadoop spark sqoop hadoop kafka sqoop spark kafka sqoop hive kafka spark sqoop kafka spark sqoop hadoop kafka sqoop']
>>> data.take(2)
['apache kafka flume karka hive ', 'kafka spark sqoop hive hadoop sqoop kafka sqoop hive spark sqoop kafka spark sqoop hadoop kafka spark sqoop hadoop hive sqoop hadoop spark kafka sqoop hadoop kafka spark sqoop hadoop kafka spark hive sqoop ']
>>> flatdata = data.flatMap(lambda x: x.split(" "))
>>> flatdata.collect()
['apache', 'kafka', 'flume', 'karka', 'hive', '', 'kafka', 'spark', 'sqoop', 'hive', 'hadoop', 'sqoop', 'kafka', 'sqoop', 'hive', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'hive', 'sqoop', 'hadoop', 'spark', 'kafka', 'sqoop', 'hadoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'spark', 'hive', 'sqoop', '', 'spark', 'hadoop', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'hadoop', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'spark', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', '', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'hadoop', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'spark', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop']
>>> flatdata.collect()
['apache', 'kafka', 'flume', 'karka', 'hive', '', 'kafka', 'spark', 'sqoop', 'hive', 'hadoop', 'sqoop', 'kafka', 'sqoop', 'hive', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'hive', 'sqoop', 'hadoop', 'spark', 'kafka', 'sqoop', 'hadoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'spark', 'hive', 'sqoop', '', 'spark', 'hadoop', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'hadoop', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'spark', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', '', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'hadoop', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'spark', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop']
>>> mapdata = flatdata.map(lambda x:x,1)
>>> mapdata.take(3)
['apache', 'kafka', 'flume']
>>> mapdata.collect()
['apache', 'kafka', 'flume', 'karka', 'hive', '', 'kafka', 'spark', 'sqoop', 'hive', 'hadoop', 'sqoop', 'kafka', 'sqoop', 'hive', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'hive', 'sqoop', 'hadoop', 'spark', 'kafka', 'sqoop', 'hadoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'spark', 'hive', 'sqoop', '', 'spark', 'hadoop', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'hadoop', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'spark', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', '', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'hadoop', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop', 'spark', 'kafka', 'sqoop', 'hive', 'kafka', 'spark', 'sqoop', 'kafka', 'spark', 'sqoop', 'hadoop', 'kafka', 'sqoop']
>>> 23/10/22 20:51:25 WARN Executor: Issue communicating with driver in heartbeater
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval
        at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)
        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)
        at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)
        at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1107)
        at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:244)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)
        at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
        at java.util.concurrent.FutureTask.runAndReset(Unknown Source)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(Unknown Source)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10000 milliseconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:259)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:314)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
        ... 13 more
23/10/22 20:51:35 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false)

>>> mapdata = flatdata.map(lambda x:(x,1))
>>> mapdata.collect()
[('apache', 1), ('kafka', 1), ('flume', 1), ('karka', 1), ('hive', 1), ('', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('hive', 1), ('hadoop', 1), ('sqoop', 1), ('kafka', 1), ('sqoop', 1), ('hive', 1), ('spark', 1), ('sqoop', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('hive', 1), ('sqoop', 1), ('hadoop', 1), ('spark', 1), ('kafka', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('spark', 1), ('hive', 1), ('sqoop', 1), ('', 1), ('spark', 1), ('hadoop', 1), ('kafka', 1), ('sqoop', 1), ('hive', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('sqoop', 1), ('hadoop', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('sqoop', 1), ('spark', 1), ('kafka', 1), ('sqoop', 1), ('hive', 1), ('kafka', 1), ('spark', 1), ('', 1), ('sqoop', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('sqoop', 1), ('hadoop', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('sqoop', 1), ('spark', 1), ('kafka', 1), ('sqoop', 1), ('hive', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('kafka', 1), ('spark', 1), ('sqoop', 1), ('hadoop', 1), ('kafka', 1), ('sqoop', 1)]
>>> reduceKey = mapdata.reduceByKey(lambda x,y : x+y)
>>> reduceKey.collect()
[('karka', 1), ('hive', 8), ('', 3), ('hadoop', 14), ('apache', 1), ('kafka', 22), ('flume', 1), ('spark', 18), ('sqoop', 26)]
>>>
